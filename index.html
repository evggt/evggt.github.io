<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder">
  <meta name="keywords" content="imitation learning, geometry-aware, visual geometry, robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="./static/images/action-tokenizer.png"/>
  <link rel="image_src" href="./static/images/action-tokenizer.png">
  <link rel="icon"
        type="image/x-icon"
        href="./static/images/evggt.ico"/>

  <title>Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-EDF010G6PN"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EDF010G6PN');


  </script>

  <script type="module"
          src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-1.11.0.min.js"></script>
  <script type="text/javascript" src="https://code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
  <script src="https://unpkg.com/interactjs/dist/interact.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" type="text/css" href="./static/slick/slick.css"/>
  <link rel="stylesheet" type="text/css" href="./static/slick/slick-theme.css"/>

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container has-text-centered">
      <h1 class="title is-1 publication-title">
        Improving Robotic Manipulation with Efficient Geometry-Aware Vision Encoder
      </h1>

<!--       <div class="column is-full_width">
        <h2 class="title is-4">IROS 2025</h2>
      </div> -->

      <div class="is-size-5 publication-authors">
        <span class="author-block">
          <a href="https://scholar.google.com/citations?user=CUpnG-YAAAAJ&hl=en">An Dinh Vuong</a><sup>1</sup>
        </span>&nbsp;&nbsp;&nbsp;
        <span class="author-block">
          <a href="https://scholar.google.com/citations?hl=th&user=qyExc4QAAAAJ">Minh Nhat Vu</a><sup>2</sup>
        </span>&nbsp;&nbsp;&nbsp;
        <span class="author-block">
          <a href="https://www.adelaide.edu.au/directory/ian.reid">Ian Reid</a><sup>1</sup>
        </span>
      </div>

      <br>
      <div class="is-size-5 publication-authors">
        <span class="author-block"><sup>1</sup>MBZUAI</span>&nbsp;&nbsp;
        <span class="author-block"><sup>2</sup>TU Wien</span>
      </div>

      <div class="column has-text-centered">
        <div class="publication-links">
          <span class="link-block">
                <a href="https://arxiv.org/abs/2503.01206" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
          <span class="link-block">
            <a href="https://arxiv.org/abs/2503.01206" class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
          <span class="link-block">
              <a href=""
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code (Coming soon)</span>
                </a>
            </span>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Existing RGB-based imitation learning approaches typically employ traditional vision encoders such as ResNet or ViT, which lack explicit 3D reasoning capabilities. Recent geometry-grounded vision models, such as VGGT~\cite{wang2025vggt}, provide robust spatial understanding and are promising candidates to address this limitation. This work investigates the integration of geometry-aware visual representations into robotic manipulation. Our results suggest that incorporating the geometry-aware vision encoder into imitation learning frameworks, including ACT and DP, yields up to 6.5% improvement over standard vision encoders in success rate across single- and bi-manual manipulation tasks in both simulation and real-world settings. Despite these benefits, most geometry-grounded models require high computational cost, limiting their deployment in practical robotic systems. To address this challenge, we propose eVGGT, an efficient geometry-aware encoder distilled from VGGT. eVGGT is nearly 9 times faster and 5 times smaller than VGGT, while preserving strong 3D reasoning capabilities. 
          </p>
        </div>
      </div>
    </div>

    <!-- Teaser video -->
    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
<!--         <video autoplay muted loop controls playsinline style="width: 100%; border-radius: 12px;">
          <source src="./static/Teaser.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video> -->
        <p class="has-text-centered" style="margin-top: 1rem;">
          <i>We present efficient geometry-aware vision encoder for improving robotic manipulation.</i>
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Non-geometry-aware vs. Geometry-aware</h2>
        <div class="content has-text-justified">
          <p>
            This paper explores the replacement of conventional 2D vision encoders in robotic manipulation with the geometry-based encoder to more effectively capture the 3D global context.
            <br><br>
            As shown in the figure below, incorporating our geometry-aware vision encoder can improve performance by up to 6.5%.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-three-fourths">
        <img src="./static/images/Teaser-policy.png" alt="Smoothness vs. Success" style="width: 70%; border-radius: 12px;" />
        <p class="has-text-centered" style="margin-top: 1rem;">
          <i>Figure: Relationship between the leveraging of geometry-aware representations and robotic success rate.</i>
        </p>
      </div>
    </div>
  </div>
</section>

  <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><strong>e</strong>fficient <strong>VGGT</strong></h2>
        <div class="content has-text-justified">
          <p>
            <strong>eVGGT</strong> is our proposed geometry,r, designed to facilitate the transfer of knowledge from geometry-aware networks. It is a lightweight variant that is 5× smaller and 9× faster, while still maintaining robust geometric reasoning capabilities.
            <br><br>
            We apply knowledge distillation to train a compressed eVGGT from a VGGT, using the same architecture but with a reduced number of transformer blocks (from 24 to 4) and substituting DINO-ViT-S for DINO-ViT-L.
          </p>
        </div>
      </div>
    </div>

    <div class="columns is-centered has-text-centered" style="margin-top: 2rem;">
      <div class="column is-four-fifths">
        <img src="./static/images/Teaser-comparison.png" alt="eVGGT comparison" style="width: 85%; border-radius: 12px;" />
        <p class="has-text-centered" style="margin-top: 1rem;">
          <i>Figure: Efficiency comparison of eVGGT and VGGT.</i>
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section" id="acknowledgements">
  <div class="container content is-max-desktop">
    <h2 class="title">Acknowledgements</h2>
    <p>
      This website template is adapted from <a href="https://hypernerf.github.io/">HyperNeRF</a>.
    </p>
  </div>
</section>

</body>
</html>





